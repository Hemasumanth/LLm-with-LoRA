# Text Classification Enhancement Using PEFT on DistilBERT

## Overview
This repository contains the work and findings of a project aimed at improving text classification using the `distilbert-base-uncased` model on the 'ag_news' dataset through Parameter-Efficient Fine-Tuning (PEFT).

## Objective
To demonstrate how PEFT can significantly boost the performance of a DistilBERT model in text classification tasks, while dramatically reducing the number of trainable parameters.

